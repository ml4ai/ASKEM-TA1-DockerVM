{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook demo's the following:\n",
    "1. Codebase -> Dynamics Linespan\n",
    "2. LLM-assisted-Codebase -> Petrinet AMR\n",
    "3. AMR enrichment with Parameter extraction\n",
    "\n",
    "Date created: 11/22/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "SKEMA_ADDRESS = os.environ.get(\"SKEMA_ADDRESS\", \"https://api.askem.lum.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebase -> Dynamics Linespan\n",
    "- Overview: \n",
    "    - This endpoint takes in a zip file containing code with dynamics somewhere in it (ideally), and returns a linespan entry for each file in it. \n",
    "- Current Details:\n",
    "    - This endpoint prompts an LLM, as a result it can be quite slow for large repo's (and other random times)\n",
    "    - As of right now, each python file in the repo will get a linespan entry. If the model does not suspect there are dynamics in that file it will output a linespan of [L0-L0] and have a description of \"Failed to parse dynamics\".\n",
    "    - Each linespan entry has a name which corresponds to the file it refers to. \n",
    "- Future Work:\n",
    "    - Adding support beyond only python, to match the same coverage as our code2fn functionality\n",
    "    - We will our developing own model for this functionality as well, it will likely be run in parallel to this unless it is significantly superior. The goal of our own model will be easier inference over a entire codebase instead of file by file like this initial support does. Also to be faster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile_path =  \"./data/code/code_sir.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"./data/code/code1/code.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = f\"{SKEMA_ADDRESS}/morae/linespan-given-filepaths-zip\"\n",
    "response_zip = requests.post(URL, files={\"zip_file\": open(zipfile_path, \"rb\")},)\n",
    "response_zip.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example contains 4 code files, each a different versio chime. One is just the dynamics, one is the complete chime model code, and other two are partial modifications of the code. Note how we get 4 responces back and dynamics was found for each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIME_SIR_URL = (\n",
    "    \"https://artifacts.askem.lum.ai/askem/data/models/zip-archives/CHIME-SIR-model.zip\"\n",
    ")\n",
    "response = requests.get(CHIME_SIR_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = f\"{SKEMA_ADDRESS}/morae/linespan-given-filepaths-zip\"\n",
    "response_zip = requests.post(URL, files={\"zip_file\": response.content},)\n",
    "response_zip.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-assisted-codebase -> Petrinet AMR\n",
    "- Overview: \n",
    "    - This endpoint takes in a zip file containing code with dynamics somewhere in it (ideally), and returns a Petrinet AMR. \n",
    "- Current Details:\n",
    "    - This endpoint has the same input and output as our /workflows/code/codebase-to-pn-amr endpoint. This is to make it easier to integrate.\n",
    "    - This endpoint takes in the codebase and uses the linespan functionality of before and slices the relevant the code which is then sent to our code-snippets endpoint. This reduces the chance for errors from our code ingestion pipeline and simplifies our extraction process as only a subset of the code is ingested, with the goal of greatly increasing the robustness of this workflow. \n",
    "    - For the case where there could multiple files with dynamics we currently only return one AMR to match the input and output of original un-assisted endpoint. To do these we return the AMR with the most \"states\", using it as a proxy for completeness. \n",
    "- Future Work:\n",
    "    - Multiple AMR outputs could be an option\n",
    "    - Expanded coverage of coding idioms\n",
    "    - Once we have our own developed linespan model up, we will replace the original endpoint with that model assisting it and run these two endpoints in parallel, unless one is significantly better than the other. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple sir\n",
    "URL = f\"{SKEMA_ADDRESS}/workflows/code/llm-assisted-codebase-to-pn-amr\"\n",
    "response_zip = requests.post(URL, files={\"zip_file\": open(zipfile_path, \"rb\")},)\n",
    "print(json.dumps(response_zip.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 CHIME models at once\n",
    "URL = f\"{SKEMA_ADDRESS}/workflows/code/llm-assisted-codebase-to-pn-amr\"\n",
    "response_zip = requests.post(URL, files={\"zip_file\": response.content},)\n",
    "print(json.dumps(response_zip.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penn chime repo, trimmed to only the python files ~49 files. \n",
    "# NOTE: Takes 3-5 minutes\n",
    "zipfile_path =  \"./data/code/chime_trimmed.zip\"\n",
    "response_zip = requests.post(URL, files={\"zip_file\": open(zipfile_path, \"rb\")},)\n",
    "print(json.dumps(response_zip.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMR enrichment with parameter extraction\n",
    "- Overview: \n",
    "    - This endpoint takes an AMR and the code it was derived from and enriches the AMR with the relevant parameters in the code. \n",
    "- Current Details:\n",
    "    - This feature checks the AMR for parameters and finds their entries in the code. It then creates a dataflow trace of their assignment and executes it to extract their value. \n",
    "    - This execution framework also gives us access to all the values a parameter can take on. However this is not output into the AMR as of now. \n",
    "- Future Work:\n",
    "    - Expanded coverage of types of assignments/executions to be extracted\n",
    "    - Handling for cases when the parameter names in the AMR do not match the variable names in the code, but they should be matched.\n",
    "    - Support for outputting parameter ranges, based on parameters that take on multiple values during execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we show enriching a simple epidemiology model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_path = Path(\"./data/execution_engine/epi_model_amr.json\")\n",
    "source_path = Path(\"./data/execution_engine/epi_model_source.py\")\n",
    "\n",
    "print(json.dumps(json.loads(amr_path.read_text()), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"amr\": json.loads(amr_path.read_text()),\n",
    "    \"source\": source_path.read_text(),\n",
    "    \"filename\": \"epi_model_source.py\"\n",
    "}\n",
    "URL = f\"{SKEMA_ADDRESS}/execution_engine/amr-enrichment\"\n",
    "response = requests.post(URL, json=request)\n",
    "enriched_amr = response.json()\n",
    "print(json.dumps(enriched_amr, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show a test file showing some of the coverage of parameter extraction capabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_path = Path(\"./data/execution_engine/complex_amr.json\")\n",
    "source_path = Path(\"./data/execution_engine/complex_source.py\")\n",
    "\n",
    "print(source_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"amr\": json.loads(amr_path.read_text()),\n",
    "    \"source\": source_path.read_text(),\n",
    "    \"filename\": \"complex_source.py\"\n",
    "}\n",
    "URL = f\"{SKEMA_ADDRESS}/execution_engine/amr-enrichment\"\n",
    "response = requests.post(URL, json=request)\n",
    "enriched_amr = response.json()\n",
    "print(json.dumps(enriched_amr, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below computes the percent coverage for the give list of coding idioms we currently support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "enriched_params = 0\n",
    "for entry in enriched_amr['semantics']['ode']['parameters']:\n",
    "    total_params+=1\n",
    "    if \"value\" in entry:\n",
    "        enriched_params+=1\n",
    "\n",
    "percent_coverage = (enriched_params/total_params) * 100\n",
    "print(f\"Percent Coverage: {percent_coverage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
