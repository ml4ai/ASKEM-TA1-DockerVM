{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some environment variables for demo purposes\n",
    "# Don't forget to install the Metadata model:\n",
    "#   pip install git+https://github.com/ml4ai/ASKEM-TA1-DataModel\n",
    "\n",
    "%env SKEMA_HOST=http://localhost:9005\n",
    "%env MIT_HOST=http://localhost:9010\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Reading Integration Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKEMA Annotation Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client code for SKEMA TR\n",
    "from typing import Any, Dict, Union\n",
    "import requests, os\n",
    "\n",
    "def annotate_text_with_skema(text:Union[str, list[str]]) -> list[Dict[str, Any]]:\n",
    "\tendpoint = f\"{os.environ['SKEMA_HOST']}/textFileToMentions\"\n",
    "\tif isinstance(text, str):\n",
    "\t\tpayload = [text] # If the text to annotate is a single string representing the contents of a document, make it a list with a single element\n",
    "\telse:\n",
    "\t\tpayload = text # if the text to annotate is already a list of documents to annotate, it is the payload itself\n",
    "\tresponse = requests.post(endpoint, json=payload, timeout=600)\n",
    "\tif response.status_code == 200:\n",
    "\t\treturn response.json()\n",
    "\telse:\n",
    "\t\traise RuntimeError(f\"Calling {endpoint} failed with HTTP code {response.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT Annotation Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client code for MIT TR\n",
    "\n",
    "def annotate_text_with_mit(texts:Union[str, list[str]]) -> list[Dict[str, Any]]:\n",
    "\tendpoint = f\"{os.environ['MIT_HOST']}/annotation/find_text_vars/\"\n",
    "\tif isinstance(texts, str):\n",
    "\t\ttexts = [texts] # If the text to annotate is a single string representing the contents of a document, make it a list with a single element\n",
    "\t\n",
    "\t# TODO paralelize this\n",
    "\treturn_values = list()\n",
    "\tfor ix, text in enumerate(texts):\n",
    "\t\tparams = {\n",
    "\t\t\t\"gpt_key\": os.environ['OPENAI_KEY'],\n",
    "\t\t\t\"text\": text\n",
    "\t\t}\n",
    "\t\tresponse = requests.post(endpoint, params=params)\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\treturn_values.append(response.json())\n",
    "\t\telse:\n",
    "\t\t\traise RuntimeError(f\"Calling {endpoint} on the {ix}th input failed with HTTP code {response.status_code}\")\n",
    "\treturn return_values\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Optional, Dict, Any\n",
    "from askem_extractions.importers import import_arizona, import_mit\n",
    "from askem_extractions.importers.mit import merge_collections\n",
    "from askem_extractions.data_model import AttributeCollection\n",
    "import itertools as it\n",
    "\n",
    "def normalize_extractions(\n",
    "        arizona_extractions:Optional[Dict[str, Any]],\n",
    "        mit_extractions:Optional[Dict]\n",
    "    ) -> AttributeCollection:\n",
    "    collections = list()\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        \n",
    "        skema_path = os.path.join(tmpdirname, \"skema.json\")\n",
    "        mit_path = os.path.join(tmpdirname, \"mit.json\")\n",
    "\n",
    "        if arizona_extractions:\n",
    "            try:\n",
    "                with open(skema_path, \"w\") as f:\n",
    "                    json.dump(arizona_extractions, f)\n",
    "                canonical_arizona = import_arizona(Path(skema_path))\n",
    "                collections.append(canonical_arizona)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "        if mit_extractions:\n",
    "            try:\n",
    "                with open(mit_path, \"w\") as f:\n",
    "                    json.dump(mit_extractions, f)\n",
    "                canonical_mit = import_mit(Path(mit_path))\n",
    "                collections.append(canonical_mit)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "        if arizona_extractions and mit_extractions:\n",
    "            # Merge both with some de de-duplications\n",
    "            params = {\n",
    "                \"gpt_key\": os.environ['OPENAI_KEY']\n",
    "            }\n",
    "\n",
    "            data = {\n",
    "                \"mit_file\": open(mit_path).read(),\n",
    "                \"arizona_file\": open(skema_path).read()\n",
    "            }\n",
    "            response = requests.post(f\"{os.environ['MIT_HOST']}/integration/get_mapping\", params=params, data=data)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                map_data = response.text()\n",
    "                map_path = os.path.join(tmpdirname, \"mapping.txt\")\n",
    "                with open(map_path, 'w')as f:\n",
    "                    f.write(map_data)\n",
    "                merged_collection = \\\n",
    "                    merge_collections(\n",
    "                        a_collection=collections[0],\n",
    "                        m_collection=collections[1],\n",
    "                        map_path=Path(map_path)\n",
    "                    )\n",
    "                \n",
    "                # Return the merged collection here\n",
    "                return merged_collection\n",
    "            \n",
    "\n",
    "    # Merge the colletions into a attribute collection\n",
    "    attributes = list(it.chain.from_iterable(c.attributes for c in collections))\n",
    "\n",
    "    return AttributeCollection(attributes=attributes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_text_extractions(texts:Union[str, list[str]]) -> AttributeCollection:\n",
    "    skema_extractions = annotate_text_with_skema(texts)\n",
    "    mit_extractions = annotate_text_with_mit(texts)\n",
    "\n",
    "    results = list()\n",
    "    for skema, mit in zip(skema_extractions, mit_extractions):\n",
    "        normalized = normalize_extractions(arizona_extractions=skema, mit_extractions=mit)\n",
    "        results.append(normalized)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a paper from the example directory\n",
    "test_text = open(\"../data/text/1-s2.0-S2211379721005490-main.txt\").read()\n",
    "\n",
    "# Run the integrated pipeline\n",
    "extractions = integrated_text_extractions(test_text)\n",
    "\n",
    "# Print the result\n",
    "for attribute in extractions[0].attributes:\n",
    "    print(attribute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make the annotations asynchronous and add flags to control which to run\n",
    "- Make equivalent functions that read COSMOS json/binaries instead of text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
